{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdpY9IiZW/x4j9GLRhA/xd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaVitali/Neuro_Symbolic_AI_example/blob/master/Neuro_SymbolicAI_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install LTNtorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgWGlbIMqh1O",
        "outputId": "088781ef-f29a-49dd-82ed-7ca4b4467faf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: LTNtorch in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from LTNtorch) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from LTNtorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->LTNtorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->LTNtorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->LTNtorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import ltn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XOZOYn7EqcvH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Krz-vFCCo1w7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/wine_dataset.csv') # load data\n",
        "df.drop('quality', axis = 1, inplace = True) # we want a binary classification problem\n",
        "df['style'] = np.where(df['style'] == 'red', True, False) # change red and withe in true or false\n",
        "df = df.sample(frac = 1) # shuffle the data taking a sample of the 100%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Class"
      ],
      "metadata": {
        "id": "T4OPpt7Ewrhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(object):\n",
        "  # constructor\n",
        "  def __init__(self, data, labels, batch_size=1, shuffle=True):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "  # return the number of batches\n",
        "  def __len__(self):\n",
        "    return int (np.ceil(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "  # describes the logic to run when we iterate over any instance of this class\n",
        "  def __iter__(self):\n",
        "    n = self.data.shape[0]\n",
        "    idx_pos = np.where(self.labels == 1)[0]\n",
        "    idx_neg = np.where(self.labels == 0)[0]\n",
        "    np.random.shuffle(idx_pos)\n",
        "    np.random.shuffle(idx_neg)\n",
        "\n",
        "    for start_idx in range(0, n, self.batch_size):\n",
        "      end_idx = min(start_idx + self.batch_size, n)\n",
        "\n",
        "      # get one positive and one negative sample for each bace, to keep the class balanced\n",
        "      pos_batch_size = min(self.batch_size // 2, len(idx_pos))\n",
        "      neg_batch_size = self.batch_size - pos_batch_size\n",
        "      pos_idx = idx_pos[:pos_batch_size]\n",
        "      neg_idx = np.random.choice(idx_neg, size = neg_batch_size, replace = False)\n",
        "\n",
        "      idx = np.concatenate([pos_idx, neg_idx])\n",
        "      np.random.shuffle(idx)\n",
        "      data = self.data[idx]\n",
        "      labels = self.labels[idx]\n",
        "      yield data, labels"
      ],
      "metadata": {
        "id": "tkgmHnJCwtoW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelA(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ModelA, self).__init__()\n",
        "\n",
        "    # model definition\n",
        "\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    self.layer1 = torch.nn.Linear(11, 64)\n",
        "    self.layer2 = torch.nn.Linear(64, 64)\n",
        "    self.layer3 = torch.nn.Linear(64, 1)\n",
        "\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.dropout  = torch.nn.Dropout(p = 0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer1(x))\n",
        "    x = self.relu(self.layer2(x))\n",
        "    x = self.dropout(x)\n",
        "    return self.sigmoid(self.layer3(x))"
      ],
      "metadata": {
        "id": "LmKWq0N05spX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logic Tensor Network (LTN)"
      ],
      "metadata": {
        "id": "D8DeKHmNsWbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to split the sataset into features and labels and then split them into training and testing sets. We standardize our feaures to have a zero mean and unit variance, thus remove scale influence (fo example the siparity between a feature measuring a person's age and another calculating their salary) and to help the NN converge faster"
      ],
      "metadata": {
        "id": "a7trgMf5syIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop('style', axis = 1).values\n",
        "features = (features - features.mean()) / features.std()"
      ],
      "metadata": {
        "id": "SErIu7_DqA01"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = torch.tensor(features).to(dtype=torch.float32)\n",
        "labels = torch.tensor(df['style'].values).to(dtype=torch.float32)"
      ],
      "metadata": {
        "id": "fYiyzJi1qoXh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the taining and testing dataloader, using a batch-size of 64. we tak the first 91 samples as part of our training set and the remaining for testing. We restrict the training set to such a small amount to higlight the power of NSAI when it comes to small data."
      ],
      "metadata": {
        "id": "jYK-QWDv3BG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(features[:91], labels[:91], 64, True)\n",
        "test_loader = DataLoader(features[91:], labels[91:], 64, False)"
      ],
      "metadata": {
        "id": "etVeDhnx25M1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the kwnoledge base and the Neural Netork Architecture\n",
        "The next thing to do now, is to extract the kwnoledge base (axioms), and create the Neural Network. we define our predicate, the connecive and the quantifiers.\n",
        "\n",
        "We define our **predicate** as a simple fedd-forward neural network of three layer:\n",
        "- input layer (12, 64), translating the dataset's 11 features to 64 neurons\n",
        "- hidden layer with 64 neurons\n",
        "- output layer converging to a single neuron\n",
        "\n",
        "we define a NOT connective and a FOR ALL quantifier. These are the recommended settings by LTNtorch for binary classification.\n"
      ],
      "metadata": {
        "id": "3RtLrdDR4GJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = ltn.Predicate(ModelA())\n",
        "\n",
        "# create the NOT standard connective\n",
        "# connective modules contributes to kwnoledge-base extraction by amalgamating aub-formulas with different features\n",
        "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
        "\n",
        "# create the FOR ALL quantifier\n",
        "# quantifier module determines the formula dimensions for tensor aggregation\n",
        "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p = 2), quantifier = \"f\")"
      ],
      "metadata": {
        "id": "oiq9OxIP70Nk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Logic tensor Network\n",
        "\n",
        "We need a way to evaluate our system, for doing so we consider two aspects:\n",
        "\n",
        "- the kwnoledge-base satisfaction level (SAT), this metric answer the question of how good the LTN is at learning. We will use this throughout the training process as part of our loss function (maximizing it)\n",
        "- the classification performance, tell us the overall performnce of the model"
      ],
      "metadata": {
        "id": "E85YLsNk9laI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sat_level(loader):\n",
        "  mean_sat = 0\n",
        "  for data, labels in loader:\n",
        "    # get positive samples\n",
        "    x_A = ltn.Variable(\"x_A\", data[torch.nonzero(labels)])\n",
        "    # get our negative samples\n",
        "    x_not_A = ltn.Variable(\"x_not_A\", data[torch.nonzero(torch.logical_not(labels))])\n",
        "\n",
        "    # get the mean SAT of both sample types\n",
        "    mean_sat += SatAgg(Forall(x_A, A(x_A)), Forall(x_not_A, Not(A(x_not_A)))\n",
        "    )\n",
        "\n",
        "  # get the mean SAT over all samples\n",
        "  mean_sat /= len(loader)\n",
        "  return mean_sat\n",
        "\n",
        "def compute_accuracy(loader):\n",
        "  mean_accuracy = 0.0\n",
        "\n",
        "  for data, labels in loader:\n",
        "    predictions = A.model(data).detach().numpy()\n",
        "\n",
        "    # convert to a binary classification\n",
        "    predictions = np.where(predictions > 0.5, 1., 0.).flatten()\n",
        "\n",
        "    # compute the accuracy score\n",
        "    mean_accuracy += accuracy_score(labels, predictions)\n",
        "\n",
        "  return mean_accuracy / len(loader)"
      ],
      "metadata": {
        "id": "s_8cRV7dmpz_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
        "optimizer = torch.optim.Adam(A.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "RaUGBeVBqeAw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS: int = 100\n",
        "\n",
        "for epoch  in range(EPOCHS):\n",
        "  # reset the training loss for every epoch\n",
        "\n",
        "  train_loss = 0.0\n",
        "\n",
        "  #start batching the data\n",
        "  for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # ground the variables with current batch data\n",
        "    x_A = ltn.Variable(\"x_A\", data[torch.nonzero(labels)])\n",
        "    x_not_A = ltn.Variable(\"x_not_A\", data[torch.nonzero(torch.logical_not(labels))])\n",
        "\n",
        "    sat_agg = SatAgg(Forall(x_A, A(x_A)),\n",
        "                      Forall(x_not_A, Not(A(x_not_A))))\n",
        "\n",
        "    # compute loss and perform backpropagation\n",
        "    loss = 1. - sat_agg\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # monitor the training loss\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "\n",
        "  if epoch % 20 == 0 :\n",
        "    print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
        "          %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
        "            compute_accuracy(train_loader), compute_accuracy(test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDaomp8srPHT",
        "outputId": "5eaad29d-4882-40d7-be4b-9fdf3228a09f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " epoch 0 | loss 0.4999 | Train Sat 0.509 | Test Sat 0.509 | Train Acc 0.562 | Test Acc 0.600\n",
            " epoch 20 | loss 0.3216 | Train Sat 0.687 | Test Sat 0.696 | Train Acc 0.930 | Test Acc 0.919\n",
            " epoch 40 | loss 0.2334 | Train Sat 0.791 | Test Sat 0.690 | Train Acc 0.945 | Test Acc 0.871\n",
            " epoch 60 | loss 0.1862 | Train Sat 0.793 | Test Sat 0.663 | Train Acc 0.945 | Test Acc 0.892\n",
            " epoch 80 | loss 0.2182 | Train Sat 0.817 | Test Sat 0.680 | Train Acc 0.961 | Test Acc 0.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_HMhab6tq4f"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}